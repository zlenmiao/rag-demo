# RAG+LLM知识库系统 - 项目详细说明

## 🚀 使用说明

### 1. 环境配置
```bash
# 安装依赖
pip install -r requirements.txt

# 配置API密钥
# 创建 key.txt 文件
# 第一行填入你的AI API密钥
# 第二行填入你的数据库链接
```

```bash
Windows下安装 Tesseract
下载：https://github.com/UB-Mannheim/tesseract/wiki
安装到以下任一路径，或其他路径时，需修改 image_processor.py possible_paths 变量：
   C:\Program Files\Tesseract-OCR\tesseract.exe
   C:\Program Files (x86)\Tesseract-OCR\tesseract.exe
   C:\Users\{用户名}\AppData\Local\Programs\Tesseract-OCR\tesseract.exe
```

### 2. 启动系统
```bash
# 启动Web服务
python web_app.py

# 访问界面
# 打开浏览器访问 http://localhost:5000
```

---

## 📋 项目架构

### 系统架构图
```
┌─────────────────────────────────────────────────────────────┐
│                    Web界面层 (Frontend)                     │
│                    templates/index.html                     │
└─────────────────────────┬───────────────────────────────────┘
                          │
┌─────────────────────────┴───────────────────────────────────┐
│                    API接口层 (Web Layer)                    │
│                      web_app.py                            │
│    ┌─────────────┬─────────────┬─────────────┬─────────────┐ │
│    │ /build      │ /query      │ /query_stream│ /stats     │ │
│    │ 构建知识库   │ 智能查询     │ 流式查询      │ 系统统计   │ │
│    └─────────────┴─────────────┴─────────────┴─────────────┘ │
└─────────────────────────┬───────────────────────────────────┘
                          │
┌─────────────────────────┴───────────────────────────────────┐
│                   业务逻辑层 (Business Layer)                │
│                      rag_system.py                         │
│    ┌─────────────┬─────────────┬─────────────┬─────────────┐ │
│    │ 知识库构建   │ 智能查询     │ 流式响应      │ 系统统计   │ │
│    └─────────────┴─────────────┴─────────────┴─────────────┘ │
└─────────────────────────┬───────────────────────────────────┘
                          │
┌─────────────────────────┴───────────────────────────────────┐
│                     核心模块层 (Core Layer)                  │
│ ┌─────────────────┬─────────────────┬─────────────────────┐ │
│ │ data_processor  │ vector_store    │ conversation_manager │ │
│ │ 数据处理模块      │ 向量存储模块     │ 对话管理模块         │ │
│ └─────────────────┴─────────────────┴─────────────────────┘ │
│ ┌─────────────────┬─────────────────┬─────────────────────┐ │
│ │ llm_client      │ data_models     │ config              │ │
│ │ LLM客户端       │ 数据模型定义     │ 系统配置             │ │
│ └─────────────────┴─────────────────┴─────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

---

## 📁 文件功能详解

### 1. 配置与模型层

#### `config.py` - 系统配置中心
**功能**：集中管理所有系统配置
- **AI配置**：支持多种大语言模型（OpenAI、DeepSeek、智谱AI、硅基流动）
- **系统设置**：向量模型、存储路径、对话历史限制等
- **提示词模板**：定义AI助手的行为规范

```python
# 配置示例
AI_CONFIGS = {
    "deepseek": {
        "api_key": "your-api-key",
        "base_url": "https://api.siliconflow.cn/v1",
        "model": "deepseek-ai/DeepSeek-V3",
        "max_tokens": 2048,
        "temperature": 0.7
    }
}
```

#### `data_models.py` - 数据模型定义
**功能**：定义系统中使用的所有数据结构
- **Document**：文档数据结构（ID、标题、内容、段落、元数据）
- **SearchResult**：搜索结果结构（文档ID、标题、相关段落、匹配分数）
- **ConversationTurn**：对话轮次结构（查询、回答、搜索结果、时间戳）
- **ConversationSession**：会话结构（会话ID、对话轮次列表、创建时间）

### 2. 数据处理层

#### `data_processor.py` - 数据处理模块
**功能**：负责文档的预处理和文本清洗
- **文本清洗**：去除特殊字符、规范化文本格式
- **段落分割**：将文档分割成有意义的段落
- **中文分词**：使用jieba进行中文分词处理
- **停用词过滤**：过滤无意义的词汇
- **文档处理**：支持txt、md等格式的文档处理

#### `vector_store.py` - 智能向量存储
**功能**：实现文档的向量化存储和智能检索
- **文档向量化**：使用Sentence-Transformers将文档转换为向量
- **质量评估**：计算段落质量分数，优化检索结果
- **智能检索**：基于余弦相似度的语义检索
- **多样性优化**：避免返回过于相似的结果
- **持久化存储**：将向量数据保存到pkl文件

```python
# 检索示例
def intelligent_search(self, query: str, top_k: int = 5) -> List[SearchResult]:
    # 1. 查询向量化
    # 2. 计算相似度
    # 3. 质量评估
    # 4. 多样性优化
    # 5. 返回最佳结果
```

### 3. 外部服务层

#### `llm_client.py` - 大语言模型客户端
**功能**：与各种大语言模型进行交互
- **多模型支持**：统一接口支持不同的LLM服务
- **流式响应**：实现实时的流式文本生成
- **错误处理**：完善的异常处理和重试机制
- **编码修复**：解决中文字符编码问题
- **连接测试**：检测API连接状态

#### `conversation_manager.py` - 对话管理器
**功能**：管理用户会话和对话历史
- **会话创建**：为每个用户创建唯一会话ID
- **历史记录**：存储对话历史，支持上下文查询
- **上下文构建**：为AI提供历史对话上下文
- **会话清理**：定期清理过期会话

### 4. 业务逻辑层

#### `rag_system.py` - 核心RAG系统
**功能**：整合所有模块，提供完整的RAG功能
- **知识库构建**：处理文档目录，构建向量知识库
- **智能查询**：结合向量检索和LLM生成答案
- **流式响应**：实现实时的流式问答
- **置信度计算**：评估答案质量和可信度
- **系统统计**：提供系统运行状态信息

**核心流程**：
1. 接收用户查询
2. 向量检索相关文档
3. 构建上下文信息
4. 调用LLM生成答案
5. 返回结果和来源信息

### 5. Web应用层

#### `web_app.py` - Flask Web应用
**功能**：提供Web API接口和前端服务
- **主页路由**：`/` - 返回前端界面
- **构建知识库**：`/build` - 构建知识库API
- **智能查询**：`/query` - 标准查询API
- **流式查询**：`/query_stream` - 流式响应API
- **系统统计**：`/stats` - 获取系统状态
- **API测试**：`/test_api` - 测试LLM连接
- **流式测试**：`/test_stream` - 测试流式响应

#### `templates/index.html` - 前端界面
**功能**：提供用户交互界面
- **现代化UI**：响应式设计，支持移动端
- **实时显示**：流式显示AI回答过程
- **功能操作**：知识库构建、查询、统计查看
- **状态反馈**：完整的处理进度提示
- **来源显示**：显示答案来源和置信度

### 6. 数据文件

#### `knowledge_base.pkl` - 知识库数据
**功能**：持久化存储向量化后的知识库
- **文档存储**：保存所有处理后的文档
- **向量数据**：存储文档段落的向量表示
- **元数据**：保存文档的附加信息
- **索引信息**：快速检索所需的索引数据

#### `requirements.txt` - 项目依赖
**功能**：定义项目所需的Python包
```
flask==2.3.3              # Web框架
jieba==0.42.1             # 中文分词
numpy==1.24.3             # 数值计算
sentence-transformers==2.2.2  # 向量化模型
scikit-learn==1.3.0       # 机器学习工具
torch==2.0.1              # 深度学习框架
transformers==4.33.2      # 自然语言处理
requests==2.31.0          # HTTP请求
```

---

## 🔄 数据流程

### 1. 知识库构建流程
```
文档目录 → 文档读取 → 文本清洗 → 段落分割 → 向量化 → 存储
    ↓
[data_processor] → [vector_store] → [knowledge_base.pkl]
```

### 2. 智能查询流程
```
用户查询 → 向量检索 → 上下文构建 → LLM生成 → 结果返回
    ↓
[vector_store] → [rag_system] → [llm_client] → [web_app]
```

### 3. 流式响应流程
```
用户查询 → 开始信号 → 检索结果 → 流式生成 → 完成统计
    ↓
[web_app] → [rag_system] → [llm_client] → [前端显示]
```

---

## 🛠️ 技术栈

### 后端技术
- **Flask**：Web框架，提供API服务
- **Sentence-Transformers**：文本向量化模型
- **NumPy**：数值计算库
- **Jieba**：中文分词库
- **Requests**：HTTP客户端
- **Pickle**：数据序列化

### 前端技术
- **HTML5/CSS3**：现代化界面
- **JavaScript**：交互逻辑
- **EventSource**：服务器推送事件
- **Fetch API**：异步数据获取

### AI技术
- **RAG**：检索增强生成
- **Transformer**：注意力机制模型
- **Vector Database**：向量数据库
- **Semantic Search**：语义检索

---

### 3. 构建知识库
1. 准备文档目录（支持txt、md格式）
2. 在界面中输入数据目录路径
3. 点击"构建知识库"按钮
4. 等待处理完成

### 4. 开始问答
1. 在输入框中输入问题
2. 点击发送按钮
3. 观察AI实时回答过程
4. 查看答案来源和置信度

---

## 📊 系统监控

### 性能指标
- **响应时间**：查询处理时间
- **置信度**：答案质量评估
- **文档数量**：知识库规模
- **查询统计**：使用情况分析

### 日志记录
- **INFO级别**：正常操作记录
- **ERROR级别**：错误信息记录
- **调试信息**：详细的处理过程

---

## 🔧 扩展说明

### 添加新的LLM服务
1. 在 `config.py` 中添加新的AI配置
2. 在 `llm_client.py` 中实现对应的API调用
3. 更新 `CURRENT_AI_SERVICE` 配置

### 支持新的文档格式
1. 在 `data_processor.py` 中添加新的文档处理逻辑
2. 扩展 `process_document` 方法
3. 添加相应的文件类型检测

### 优化检索算法
1. 修改 `vector_store.py` 中的检索逻辑
2. 调整相似度计算方法
3. 优化质量评估算法

---

*最后更新：2025年07月*